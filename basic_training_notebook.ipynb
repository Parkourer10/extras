{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r11cNiLqvWC6"
   },
   "source": [
    "# Training a microWakeWord Model\n",
    "\n",
    "This notebook steps you through training a basic microWakeWord model. It is intended as a **starting point** for advanced users. You should use Python 3.10.\n",
    "\n",
    "**The model generated will most likely not be usable for everyday use; it may be difficult to trigger or falsely activates too frequently. You will most likely have to experiment with many different settings to obtain a decent model!**\n",
    "\n",
    "In the comment at the start of certain blocks, I note some specific settings to consider modifying.\n",
    "\n",
    "This runs on Google Colab, but is extremely slow compared to training on a local GPU. If you must use Colab, be sure to Change the runtime type to a GPU. Even then, it still slow!\n",
    "\n",
    "At the end of this notebook, you will be able to download a tflite file. To use this in ESPHome, you need to write a model manifest JSON file. See the [ESPHome documentation](https://esphome.io/components/micro_wake_word) for the details and the [model repo](https://github.com/esphome/micro-wake-word-models/tree/main/models/v2) for examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BFf6511E65ff"
   },
   "outputs": [],
   "source": [
    "# Installs microWakeWord. Be sure to restart the session after this is finished.\n",
    "import platform\n",
    "!pip install edge-tts\n",
    "\n",
    "if platform.system() == \"Darwin\":\n",
    "    # `pymicro-features` is installed from a fork to support building on macOS\n",
    "    !pip install 'git+https://github.com/puddly/pymicro-features@puddly/minimum-cpp-version'\n",
    "\n",
    "# `audio-metadata` is installed from a fork to unpin `attrs` from a version that breaks Jupyter\n",
    "!pip install 'git+https://github.com/whatsnowplaying/audio-metadata@d4ebb238e6a401bb1a5aaaac60c9e2b3cb30929f'\n",
    "\n",
    "!git clone https://github.com/kahrendt/microWakeWord\n",
    "!pip install -e ./microWakeWord\n",
    "!pip install piper piper-tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/rhasspy/piper-voices/resolve/main/en/en_US/amy/medium/en_US-amy-medium.onnx\n",
    "!wget https://huggingface.co/rhasspy/piper-voices/resolve/main/en/en_US/amy/medium/en_US-amy-medium.onnx.json\n",
    "!wget https://huggingface.co/rhasspy/piper-voices/resolve/main/en/en_US/kusal/medium/en_US-kusal-medium.onnx\n",
    "!wget https://huggingface.co/rhasspy/piper-voices/resolve/main/en/en_US/kusal/medium/en_US-kusal-medium.onnx.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dEluu7nL7ywd"
   },
   "outputs": [],
   "source": [
    "# Generates 1 sample of the target word for manual verification.\n",
    "\n",
    "target_word = 'hey bob'  # Phonetic spellings may produce better samples\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "from IPython.display import Audio\n",
    "\n",
    "if not os.path.exists(\"./piper-sample-generator\"):\n",
    "    if platform.system() == \"Darwin\":\n",
    "        !git clone -b mps-support https://github.com/kahrendt/piper-sample-generator\n",
    "    else:\n",
    "        !git clone https://github.com/rhasspy/piper-sample-generator\n",
    "\n",
    "    !wget -O piper-sample-generator/models/en_US-libritts_r-medium.pt 'https://github.com/rhasspy/piper-sample-generator/releases/download/v2.0.0/en_US-libritts_r-medium.pt'\n",
    "\n",
    "    # Install system dependencies\n",
    "    !pip install torch torchaudio piper-phonemize-cross==1.2.1\n",
    "\n",
    "    if \"piper-sample-generator/\" not in sys.path:\n",
    "        sys.path.append(\"piper-sample-generator/\")\n",
    "\n",
    "!python3 piper-sample-generator/generate_samples.py \"{target_word}\" \\\n",
    "--max-samples 1 \\\n",
    "--batch-size 1 \\\n",
    "--output-dir generated_samples\n",
    "\n",
    "\n",
    "Audio(\"generated_samples/0.wav\", autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-SvGtCCM9akR"
   },
   "outputs": [],
   "source": [
    "# Generates a larger amount of wake word samples.\n",
    "# Start here when trying to improve your model.\n",
    "# See https://github.com/rhasspy/piper-sample-generator for the full set of\n",
    "# parameters. In particular, experiment with noise-scales and noise-scale-ws,\n",
    "# generating negative samples similar to the wake word, and generating many more\n",
    "# wake word samples, possibly with different phonetic pronunciations.\n",
    "\n",
    "# 1. Positive samples for \"bob\"\n",
    "!python3 piper-sample-generator/generate_samples.py \"hey bob\" \\\n",
    "  --max-samples 10000 \\\n",
    "  --batch-size 500 \\\n",
    "  --noise-scales 0.5 1.0 \\\n",
    "  --noise-scale-ws 0.5 1.0 \\\n",
    "  --output-dir /pee/generated_samples\n",
    "\n",
    "!python3 piper-sample-generator/generate_samples.py \"pop rob bop bab mob kop lop dop bok zop aop alexa google cat dog hello start stop boob\" \\\n",
    "  --max-samples 5000 \\\n",
    "  --batch-size 500 \\\n",
    "  --noise-scales 0.5 1.0 \\\n",
    "  --noise-scale-ws 0.5 1.0 \\\n",
    "  --output-dir /pee/pee\n",
    "\n",
    "import edge_tts\n",
    "import asyncio\n",
    "import os\n",
    "import subprocess\n",
    "import random\n",
    "\n",
    "output_dir = \"/pee/validation\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Diverse voices across accents\n",
    "voices = [\n",
    "    \"en-US-AriaNeural\", \"en-US-GuyNeural\", \"en-US-JennyNeural\", \"en-US-TonyNeural\",\n",
    "    \"en-GB-LibbyNeural\", \"en-GB-RyanNeural\", \"en-GB-SoniaNeural\", \"en-GB-ThomasNeural\",\n",
    "    \"en-IN-NeerjaNeural\", \"en-IN-PrabhatNeural\",\n",
    "    \"en-AU-NatashaNeural\", \"en-AU-WilliamNeural\",\n",
    "    \"en-IE-ConnorNeural\", \"en-IE-EmilyNeural\",\n",
    "    \"en-CA-ClaraNeural\", \"en-CA-LiamNeural\",\n",
    "    \"en-ZA-LeahNeural\", \"en-ZA-LukeNeural\",\n",
    "    \"en-NG-EzinneNeural\", \"en-NG-AbeoNeural\",\n",
    "    \"en-NZ-MitchellNeural\", \"en-NZ-MollyNeural\",\n",
    "    \"en-PH-JamesNeural\", \"en-PH-RosaNeural\",\n",
    "    \"en-KE-AsiliaNeural\", \"en-KE-ChilembaNeural\",\n",
    "    \"en-SG-LunaNeural\", \"en-SG-WayneNeural\",\n",
    "]\n",
    "\n",
    "SAMPLES_PER_VOICE = 50  # 50 Ã— ~30 voices = 1500 samples\n",
    "\n",
    "async def main():\n",
    "    idx = 0\n",
    "    for voice in voices:\n",
    "        for j in range(SAMPLES_PER_VOICE):\n",
    "            tmp_path = os.path.join(output_dir, f\"tmp_{idx}.mp3\")\n",
    "            wav_path = os.path.join(output_dir, f\"bob_{idx}_{voice}.wav\")\n",
    "\n",
    "            # Random pitch/rate for natural variation\n",
    "            rate = f\"{random.randint(-20, 20)}%\"    # -20% to +20%\n",
    "            pitch = f\"{random.randint(-10, 10)}Hz\" # -10Hz to +10Hz\n",
    "\n",
    "            ssml_text = f\"<speak><prosody rate='{rate}' pitch='{pitch}'>bob</prosody></speak>\"\n",
    "\n",
    "            communicate = edge_tts.Communicate(ssml_text, voice)\n",
    "            await communicate.save(tmp_path)\n",
    "\n",
    "            # Convert to 16kHz mono WAV\n",
    "            subprocess.run([\n",
    "                \"ffmpeg\", \"-y\", \"-i\", tmp_path,\n",
    "                \"-ar\", \"16000\", \"-ac\", \"1\", wav_path\n",
    "            ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "            os.remove(tmp_path)\n",
    "            print(f\"Saved {wav_path}\")\n",
    "            idx += 1\n",
    "\n",
    "asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJRG4Qvo9nXG"
   },
   "outputs": [],
   "source": [
    "# Downloads audio data for augmentation. This can be slow!\n",
    "# Adapted from openWakeWord's automatic_model_training.ipynb\n",
    "\n",
    "import datasets\n",
    "import scipy\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "## ----------------------------\n",
    "## Download MIT RIR data\n",
    "## ----------------------------\n",
    "\n",
    "output_dir = \"/pee/mit_rirs\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "    rir_dataset = datasets.load_dataset(\"davidscripka/MIT_environmental_impulse_responses\", split=\"train\", streaming=True)\n",
    "    # Save clips to 16-bit PCM wav files\n",
    "    for row in tqdm(rir_dataset):\n",
    "        name = row['audio']['path'].split('/')[-1]\n",
    "        scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))\n",
    "\n",
    "## ----------------------------\n",
    "## Download noise and background audio\n",
    "## ----------------------------\n",
    "\n",
    "# AudioSet Dataset (https://research.google.com/audioset/dataset/index.html)\n",
    "# HuggingFace mirror: https://huggingface.co/datasets/agkphysics/AudioSet\n",
    "\n",
    "if not os.path.exists(\"/pee/audioset\"):\n",
    "    os.mkdir(\"/pee/audioset\")\n",
    "\n",
    "    # Grab two parts of the dataset instead of one\n",
    "    for fname in [\"bal_train09.tar\", \"bal_train08.tar\"]:\n",
    "        out_dir = f\"audioset/{fname}\"\n",
    "        link = \"https://huggingface.co/datasets/agkphysics/AudioSet/resolve/main/data/\" + fname\n",
    "        !wget -O {out_dir} {link}\n",
    "        !cd audioset && tar -xf {fname}\n",
    "\n",
    "    output_dir = \"/pee/audioset_16k\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "\n",
    "    # Save clips to 16-bit PCM wav files\n",
    "    audioset_dataset = datasets.Dataset.from_dict({\"audio\": [str(i) for i in Path(\"audioset/audio\").glob(\"**/*.flac\")]})\n",
    "    audioset_dataset = audioset_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000))\n",
    "    for row in tqdm(audioset_dataset):\n",
    "        name = row['audio']['path'].split('/')[-1].replace(\".flac\", \".wav\")\n",
    "        scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))\n",
    "\n",
    "# Free Music Archive dataset (extra small subset)\n",
    "# https://github.com/mdeff/fma\n",
    "\n",
    "output_dir = \"/pee/fma\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "    fname = \"fma_xs.zip\"\n",
    "    link = \"https://huggingface.co/datasets/mchl914/fma_xsmall/resolve/main/\" + fname\n",
    "    out_dir = f\"fma/{fname}\"\n",
    "    !wget -O {out_dir} {link}\n",
    "    !cd {output_dir} && unzip -q {fname}\n",
    "\n",
    "    output_dir = \"/pee/fma_16k\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "\n",
    "    # Save clips to 16-bit PCM wav files\n",
    "    fma_dataset = datasets.Dataset.from_dict({\"audio\": [str(i) for i in Path(\"fma/fma_small\").glob(\"**/*.mp3\")]})\n",
    "    fma_dataset = fma_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000))\n",
    "    for row in tqdm(fma_dataset):\n",
    "        name = row['audio']['path'].split('/')[-1].replace(\".mp3\", \".wav\")\n",
    "        scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))\n",
    "\n",
    "\n",
    "output_dir = \"/pee/fsd50k\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Stream dataset (does not download all at once)\n",
    "fsd_dataset = datasets.load_dataset(\"Fhrozen/FSD50k\", split=\"train\", streaming=True)\n",
    "fsd_dataset = fsd_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000))\n",
    "# Download and save only 10k clips\n",
    "for idx, row in enumerate(tqdm(fsd_dataset, total=10000)):\n",
    "    if idx >= 10000:   # <-- stop after 10k\n",
    "        break\n",
    "    name = f\"fsd50k_{idx}.wav\"\n",
    "    scipy.io.wavfile.write(\n",
    "        os.path.join(output_dir, name),\n",
    "        16000,\n",
    "        (row[\"audio\"][\"array\"] * 32767).astype(np.int16)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XW3bmbI5-JAz"
   },
   "outputs": [],
   "source": [
    "# Sets up the augmentations.\n",
    "# To improve your model, experiment with these settings and use more sources of\n",
    "# background clips.\n",
    "\n",
    "from microwakeword.audio.augmentation import Augmentation\n",
    "from microwakeword.audio.clips import Clips\n",
    "from microwakeword.audio.spectrograms import SpectrogramGeneration\n",
    "\n",
    "# Positive wake word samples (\"bob\")\n",
    "clips = Clips(input_directory='/pee/generated_samples',\n",
    "              file_pattern='*.wav',\n",
    "              max_clip_duration_s=None,\n",
    "              remove_silence=False,\n",
    "              random_split_seed=10,\n",
    "              split_count=0.1,\n",
    "              )\n",
    "\n",
    "augmenter = Augmentation(augmentation_duration_s=3.2,\n",
    "                         augmentation_probabilities={\n",
    "                                \"SevenBandParametricEQ\": 0.6,   # most get EQ\n",
    "                                \"TanhDistortion\": 0.3,          # distortion fairly often\n",
    "                                \"PitchShift\": 0.4,              # many get pitch shifts\n",
    "                                \"BandStopFilter\": 0.3,          # frequent filtering\n",
    "                                \"AddColorNoise\": 0.5,           # about half get extra noise\n",
    "                                \"AddBackgroundNoise\": 0.8,      # almost all get bg noise\n",
    "                                \"Gain\": 0.7,                    # gain adjustment most times\n",
    "                                \"RIR\": 0.5,                     # half get reverb\n",
    "                            },\n",
    "                         impulse_paths=['mit_rirs'],\n",
    "                         background_paths=['fma_16k', 'audioset_16k', 'fsd50k'],\n",
    "                         background_min_snr_db=-5,\n",
    "                         background_max_snr_db=10,\n",
    "                         min_jitter_s=0.195,\n",
    "                         max_jitter_s=0.205,\n",
    "                         )\n",
    "\n",
    "# Negative samples (\"pee\" folder)\n",
    "clips_neg = Clips(input_directory='/pee/pee',\n",
    "                  file_pattern='*.wav',\n",
    "                  max_clip_duration_s=None,\n",
    "                  remove_silence=False,\n",
    "                  random_split_seed=10,\n",
    "                  split_count=0.1,\n",
    "                  )\n",
    "\n",
    "augmenter_neg = Augmentation(augmentation_duration_s=3.2,\n",
    "                              augmentation_probabilities={\n",
    "                                    \"SevenBandParametricEQ\": 0.6,\n",
    "                                    \"TanhDistortion\": 0.3,\n",
    "                                    \"PitchShift\": 0.4,\n",
    "                                    \"BandStopFilter\": 0.3,\n",
    "                                    \"AddColorNoise\": 0.5,\n",
    "                                    \"AddBackgroundNoise\": 0.8,\n",
    "                                    \"Gain\": 0.7,\n",
    "                                    \"RIR\": 0.5,\n",
    "                              },\n",
    "                              impulse_paths=['mit_rirs'],\n",
    "                              background_paths=['fma_16k', 'audioset_16k', 'fsd50k'],\n",
    "                              background_min_snr_db=-5,\n",
    "                              background_max_snr_db=10,\n",
    "                              min_jitter_s=0.195,\n",
    "                              max_jitter_s=0.205,\n",
    "                              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V5UsJfKKD1k9"
   },
   "outputs": [],
   "source": [
    "# Augment a random clip and play it back to verify it works well\n",
    "\n",
    "from IPython.display import Audio\n",
    "from microwakeword.audio.audio_utils import save_clip\n",
    "\n",
    "random_clip = clips.get_random_clip()\n",
    "augmented_clip = augmenter.augment_clip(random_clip)\n",
    "save_clip(augmented_clip, 'augmented_clip.wav')\n",
    "\n",
    "Audio(\"augmented_clip.wav\", autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D7BHcY1mEGbK"
   },
   "outputs": [],
   "source": [
    "# Augment samples and save the training, validation, and testing sets.\n",
    "# Positive = generated_samples/\n",
    "# Negative = pee/\n",
    "# Validation = validation/ (pre-generated clean TTS voices, no splitting)\n",
    "\n",
    "import os\n",
    "from mmap_ninja.ragged import RaggedMmap\n",
    "from microwakeword.audio.spectrograms import SpectrogramGeneration\n",
    "\n",
    "output_dir = 'generated_augmented_features'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ---------------------------\n",
    "# Positive spectrograms\n",
    "# ---------------------------\n",
    "def process_split(split, clips, augmenter, tag, external_validation=False):\n",
    "    \"\"\"Helper to generate mmap features for a given dataset (positive/negative).\"\"\"\n",
    "\n",
    "    out_dir = os.path.join(output_dir, split)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    split_name = \"train\"\n",
    "    repetition = 2\n",
    "    slide_frames = 10\n",
    "\n",
    "    # Validation/test settings\n",
    "    if split == \"validation\":\n",
    "        split_name = \"validation\"\n",
    "        repetition = 1\n",
    "    elif split == \"testing\":\n",
    "        split_name = \"test\"\n",
    "        repetition = 1\n",
    "        slide_frames = 1\n",
    "\n",
    "    # External validation (real TTS voices): skip augmenter\n",
    "    if external_validation:\n",
    "        spectrograms = SpectrogramGeneration(\n",
    "            clips=clips,\n",
    "            augmenter=None,\n",
    "            slide_frames=slide_frames,\n",
    "            step_ms=10,\n",
    "        )\n",
    "    else:\n",
    "        spectrograms = SpectrogramGeneration(\n",
    "            clips=clips,\n",
    "            augmenter=augmenter,\n",
    "            slide_frames=slide_frames,\n",
    "            step_ms=10,\n",
    "        )\n",
    "\n",
    "    RaggedMmap.from_generator(\n",
    "        out_dir=os.path.join(out_dir, f'{tag}_mmap'),\n",
    "        sample_generator=spectrograms.spectrogram_generator(split=split_name, repeat=repetition),\n",
    "        batch_size=100,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "\n",
    "# ---------------------------O\n",
    "# Generate training/val/test\n",
    "# ---------------------------\n",
    "\n",
    "splits = [\"training\", \"validation\", \"testing\"]\n",
    "\n",
    "# Positive samples\n",
    "for split in splits:\n",
    "    process_split(split, clips, augmenter, \"wakeword\")\n",
    "\n",
    "# Negative samples\n",
    "for split in splits:\n",
    "    process_split(split, clips_neg, augmenter_neg, \"negative\")\n",
    "\n",
    "# Real TTS validation (clean voices in \"validation/\" dir, no augmentation)\n",
    "from microwakeword.audio.clips import Clips\n",
    "tts_validation = Clips(input_directory='/pee/validation', file_pattern='*.wav')\n",
    "process_split(\"validation\", tts_validation, None, \"tts_validation\", external_validation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1pGuJDPyp3ax"
   },
   "outputs": [],
   "source": [
    "# Downloads pre-generated spectrogram features (made for microWakeWord in\n",
    "# particular) for various negative datasets. This can be slow!\n",
    "\n",
    "output_dir = '/pee/pee'\n",
    "if os.path.exists(output_dir):\n",
    "    print('im working negative prt')\n",
    "    link_root = \"https://huggingface.co/datasets/kahrendt/microwakeword/resolve/main/\"\n",
    "    filenames = ['dinner_party.zip', 'dinner_party_eval.zip', 'no_speech.zip', 'speech.zip', 'speech_background.zip']\n",
    "    for fname in filenames:\n",
    "        link = link_root + fname\n",
    "\n",
    "        zip_path = f\"/pee/pee/{fname}\"\n",
    "        !wget -O {zip_path} {link}\n",
    "        !unzip -q {zip_path} -d {output_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ii1A14GsGVQT"
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "config = {}\n",
    "\n",
    "config[\"window_step_ms\"] = 10\n",
    "config[\"train_dir\"] = \"/pee/trained_models/wakeword\"\n",
    "\n",
    "config[\"features\"] = [\n",
    "    {\n",
    "        # Training positives\n",
    "        \"features_dir\": \"generated_augmented_features/training\",\n",
    "        \"sampling_weight\": 2.0,\n",
    "        \"penalty_weight\": 1.0,\n",
    "        \"truth\": True,\n",
    "        \"truncation_strategy\": \"truncate_start\",\n",
    "        \"type\": \"mmap\",\n",
    "    },\n",
    "    {\n",
    "        # Validation positives (TTS Bob, 90% clean + 10% augmented)\n",
    "        \"features_dir\": \"generated_augmented_features/validation\",\n",
    "        \"sampling_weight\": 0.0,   # eval only\n",
    "        \"penalty_weight\": 1.0,\n",
    "        \"truth\": True,\n",
    "        \"truncation_strategy\": \"truncate_start\",\n",
    "        \"type\": \"mmap\",\n",
    "    },\n",
    "    {\n",
    "        # Testing positives (streaming, clean)\n",
    "        \"features_dir\": \"generated_augmented_features/testing\",\n",
    "        \"sampling_weight\": 0.0,   # eval only\n",
    "        \"penalty_weight\": 1.0,\n",
    "        \"truth\": True,\n",
    "        \"truncation_strategy\": \"truncate_start\",\n",
    "        \"type\": \"mmap\",\n",
    "    },\n",
    "    {\n",
    "        \"features_dir\": \"/pee/pee/speech\",\n",
    "        \"sampling_weight\": 10.0,\n",
    "        \"penalty_weight\": 1.0,\n",
    "        \"truth\": False,\n",
    "        \"truncation_strategy\": \"random\",\n",
    "        \"type\": \"mmap\",\n",
    "    },\n",
    "    {\n",
    "        \"features_dir\": \"/pee/pee/dinner_party\",\n",
    "        \"sampling_weight\": 10.0,\n",
    "        \"penalty_weight\": 1.0,\n",
    "        \"truth\": False,\n",
    "        \"truncation_strategy\": \"random\",\n",
    "        \"type\": \"mmap\",\n",
    "    },\n",
    "    {\n",
    "        \"features_dir\": \"/pee/pee/no_speech\",\n",
    "        \"sampling_weight\": 5.0,\n",
    "        \"penalty_weight\": 1.0,\n",
    "        \"truth\": False,\n",
    "        \"truncation_strategy\": \"random\",\n",
    "        \"type\": \"mmap\",\n",
    "    },\n",
    "    { # Ambient eval negatives (for validation/testing false alarms)\n",
    "        \"features_dir\": \"/pee/pee/dinner_party_eval\",\n",
    "        \"sampling_weight\": 0.0,   # eval only\n",
    "        \"penalty_weight\": 1.0,\n",
    "        \"truth\": False,\n",
    "        \"truncation_strategy\": \"split\",\n",
    "        \"type\": \"mmap\",\n",
    "    },\n",
    "    { # Extra validation_ambient set for silent/noisy conditions\n",
    "        \"features_dir\": \"/pee/pee/no_speech\",\n",
    "        \"sampling_weight\": 0.0,   # eval only\n",
    "        \"penalty_weight\": 1.0,\n",
    "        \"truth\": False,\n",
    "        \"truncation_strategy\": \"split\",\n",
    "        \"type\": \"mmap\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Training schedule\n",
    "config[\"training_steps\"] = [20000]\n",
    "config[\"positive_class_weight\"] = [1]\n",
    "config[\"negative_class_weight\"] = [20]\n",
    "\n",
    "config[\"learning_rates\"] = [0.001]\n",
    "config[\"batch_size\"] = 64\n",
    "\n",
    "# SpecAugment disabled\n",
    "config[\"time_mask_max_size\"] = [0]\n",
    "config[\"time_mask_count\"] = [0]\n",
    "config[\"freq_mask_max_size\"] = [0]\n",
    "config[\"freq_mask_count\"] = [0]\n",
    "\n",
    "# Eval settings\n",
    "config[\"eval_step_interval\"] = 500\n",
    "config[\"clip_duration_ms\"] = 1500\n",
    "\n",
    "# Model selection criteria\n",
    "config[\"target_minimization\"] = 0.9\n",
    "config[\"minimization_metric\"] = None\n",
    "config[\"maximization_metric\"] = \"average_viable_recall\"\n",
    "\n",
    "with open(os.path.join(\"training_parameters.yaml\"), \"w\") as file:\n",
    "    yaml.dump(config, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WoEXJBaiC9mf"
   },
   "outputs": [],
   "source": [
    "# Trains a model. When finished, it will quantize and convert the model to a\n",
    "# streaming version suitable for on-device detection.\n",
    "# It will resume if stopped, but it will start over at the configured training\n",
    "# steps in the yaml file.\n",
    "# Change --train 0 to only convert and test the best-weighted model.\n",
    "# On Google colab, it doesn't print the mini-batch results, so it may appear\n",
    "# stuck for several minutes! Additionally, it is very slow compared to training\n",
    "# on a local GPU.\n",
    "\n",
    "!python -m microwakeword.model_train_eval \\\n",
    "--training_config='training_parameters.yaml' \\\n",
    "--train 0 \\\n",
    "--restore_checkpoint 1 \\\n",
    "--test_tf_nonstreaming 0 \\\n",
    "--test_tflite_nonstreaming 0 \\\n",
    "--test_tflite_nonstreaming_quantized 0 \\\n",
    "--test_tflite_streaming 0 \\\n",
    "--test_tflite_streaming_quantized 1 \\\n",
    "--use_weights \"best_weights\" \\\n",
    "mixednet \\\n",
    "--pointwise_filters \"64,64,64,64\" \\\n",
    "--repeat_in_block  \"1, 1, 1, 1\" \\\n",
    "--mixconv_kernel_sizes '[5], [7,11], [9,15], [23]' \\\n",
    "--residual_connection \"0,0,0,0\" \\\n",
    "--first_conv_filters 32 \\\n",
    "--first_conv_kernel_size 5 \\\n",
    "--stride 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ex_UIWvwtjAN"
   },
   "outputs": [],
   "source": [
    "# Downloads the tflite model file. To use on the device, you need to write a\n",
    "# Model JSON file. See https://esphome.io/components/micro_wake_word for the\n",
    "# documentation and\n",
    "# https://github.com/esphome/micro-wake-word-models/tree/main/models/v2 for\n",
    "# examples. Adjust the probability threshold based on the test results obtained\n",
    "# after training is finished. You may also need to increase the Tensor arena\n",
    "# model size if the model fails to load.\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "files.download(f\"/pee/trained_models/wakeword/tflite_stream_state_internal_quant/stream_state_internal_quant.tflite\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
